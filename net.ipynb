{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import itertools as IT\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "file = open('dev-0/in.tsv', 'r', encoding='utf8').readlines()\n",
    "data = []\n",
    "for line in file:\n",
    "    line = line.lower()\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    new_words = tokenizer.tokenize(line)\n",
    "    data.append(new_words)\n",
    "\n",
    "file = open('dev-1/in.tsv', 'r', encoding='utf8').readlines()\n",
    "for line in file:\n",
    "    line = line.lower()\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    new_words = tokenizer.tokenize(line)\n",
    "    data.append(new_words)\n",
    "\n",
    "file = open('test-A/in.tsv', 'r', encoding='utf8').readlines()\n",
    "for line in file:\n",
    "    line = line.lower()\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    new_words = tokenizer.tokenize(line)\n",
    "    data.append(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('train/in.tsv', 'r', encoding='utf8').readlines()[0:5000]\n",
    "train_data = []\n",
    "for line in file:\n",
    "    line = line.lower()\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    new_words = tokenizer.tokenize(line)\n",
    "    train_data.append(new_words)\n",
    "\n",
    "file = open('train/expected.tsv', 'r', encoding='utf8').readlines()[0:5000]\n",
    "train_data_output = []\n",
    "for line in file:\n",
    "    train_data_output.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {}\n",
    "for line in data + train_data:\n",
    "    for word in line:\n",
    "        if word not in my_dict:\n",
    "            my_dict[word] = len(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_SIZE = len(my_dict)\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(DICT_SIZE, 2)\n",
    "    def forward(self, x):\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "    \n",
    "def make_vector(sentence, words):\n",
    "    vector = torch.zeros(len(words))\n",
    "    for word in sentence:\n",
    "        vector[words[word]] += 1\n",
    "    return vector.view(1, -1)\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "opt = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    if epoch % 10 == 0:\n",
    "        print(str(epoch/epochs * 100) + \"%\")\n",
    "    for instance, label in IT.zip_longest(train_data, train_data_output):\n",
    "        vector = make_vector(instance, my_dict)\n",
    "        target = torch.LongTensor([label])\n",
    "        model.zero_grad()\n",
    "        log_probs = model(vector)\n",
    "        loss = criterion(log_probs, target)\n",
    "        loss.backward()\n",
    "        opt.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('dev-0/in.tsv', 'r', encoding='utf8').readlines()\n",
    "dev_data = []\n",
    "for line in file:\n",
    "    line = line.lower()\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    new_words = tokenizer.tokenize(line)\n",
    "    dev_data.append(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('dev-0/out.tsv','w')\n",
    "for line in dev_data:\n",
    "        vec = make_vector(line, my_dict)\n",
    "        log_probs = model(vec)\n",
    "        y_pred = np.argmax(log_probs[0].detach().numpy())\n",
    "        f.write(str(int(y_pred)) + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('dev-1/in.tsv', 'r', encoding='utf8').readlines()\n",
    "dev_data = []\n",
    "for line in file:\n",
    "    line = line.lower()\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    new_words = tokenizer.tokenize(line)\n",
    "    dev_data.append(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('dev-1/out.tsv','w')\n",
    "for line in dev_data:\n",
    "        vec = make_vector(line, my_dict)\n",
    "        log_probs = model(vec)\n",
    "        y_pred = np.argmax(log_probs[0].detach().numpy())\n",
    "        f.write(str(int(y_pred)) + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('test-A/in.tsv', 'r', encoding='utf8').readlines()\n",
    "dev_data = []\n",
    "for line in file:\n",
    "    line = line.lower()\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    new_words = tokenizer.tokenize(line)\n",
    "    dev_data.append(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('test-A/out.tsv','w')\n",
    "for line in dev_data:\n",
    "        vec = make_vector(line, my_dict)\n",
    "        log_probs = model(vec)\n",
    "        y_pred = np.argmax(log_probs[0].detach().numpy())\n",
    "        f.write(str(int(y_pred)) + '\\n')\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
